package handlers

import (
	"fmt"
	"start-feishubot/services"
	"start-feishubot/services/openai"
	"strings"
)

type ModelAction struct{}

func (*ModelAction) Execute(a *ActionInfo) bool {
	cmd := a.info.qParsed
	
	// æ£€æŸ¥å‘½ä»¤ç±»å‹
	if strings.HasPrefix(cmd, "/model") || strings.HasPrefix(cmd, "/æ¨¡å‹") {
		handleModelSwitch(a)
		return false // é˜»æ­¢åç»­å¤„ç†
	}
	
	if strings.HasPrefix(cmd, "/compare") || strings.HasPrefix(cmd, "/å¯¹æ¯”") {
		handleModelCompare(a)
		return false // é˜»æ­¢åç»­å¤„ç†
	}
	
	if strings.HasPrefix(cmd, "/models") || strings.HasPrefix(cmd, "/æ¨¡å‹åˆ—è¡¨") {
		handleModelList(a)
		return false // é˜»æ­¢åç»­å¤„ç†
	}
	
	if strings.HasPrefix(cmd, "/current") || strings.HasPrefix(cmd, "/å½“å‰æ¨¡å‹") {
		handleCurrentModel(a)
		return false // é˜»æ­¢åç»­å¤„ç†
	}
	
	return true // ç»§ç»­å¤„ç†å…¶ä»–æ¶ˆæ¯
}

// handleModelSwitch å¤„ç†æ¨¡å‹åˆ‡æ¢å‘½ä»¤
func handleModelSwitch(a *ActionInfo) bool {
	cmd := a.info.qParsed
	
	// è§£æå‘½ä»¤å‚æ•°
	parts := strings.Fields(cmd)
	if len(parts) < 2 {
		sendModelHelp(a)
		return true
	}
	
	modelQuery := parts[1]
	sessionId := *a.info.sessionId
	
	// æœç´¢åŒ¹é…çš„æ¨¡å‹
	var selectedModel *openai.ModelInfo
	
	// ç›´æ¥åŒ¹é…æ¨¡å‹ID
	if model, exists := openai.SupportedModels[modelQuery]; exists {
		selectedModel = model
	} else {
		// æ¨¡ç³Šæœç´¢
		searchResults := openai.SearchModels(modelQuery)
		if len(searchResults) == 1 {
			selectedModel = searchResults[0]
		} else if len(searchResults) > 1 {
			sendModelSearchResults(a, searchResults, modelQuery)
			return true
		}
	}
	
	if selectedModel == nil {
		sendModelNotFound(a, modelQuery)
		return true
	}
	
	// åˆ‡æ¢æ¨¡å‹
	services.GetSessionCache().SetCurrentModel(sessionId, selectedModel.ID)
	services.GetSessionCache().SetCompareMode(sessionId, false)
	
	// å‘é€ç¡®è®¤æ¶ˆæ¯
	confirmMsg := fmt.Sprintf("âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: %s\n\n%s", 
		selectedModel.Name, selectedModel.Description)
	replyMsg(*a.ctx, confirmMsg, a.info.msgId)
	
	return true
}

// handleModelCompare å¤„ç†å¤šæ¨¡å‹å¯¹æ¯”å‘½ä»¤
func handleModelCompare(a *ActionInfo) bool {
	cmd := a.info.qParsed
	sessionId := *a.info.sessionId
	
	// è§£æå‘½ä»¤
	parts := strings.SplitN(cmd, " ", 2)
	if len(parts) < 2 {
		sendCompareHelp(a)
		return true
	}
	
	query := parts[1]
	
	// å¯ç”¨å¯¹æ¯”æ¨¡å¼
	services.GetSessionCache().SetCompareMode(sessionId, true)
	
	// è·å–æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
	allModels := openai.GetAllModels()
	
	// å‘é€åˆå§‹æ¶ˆæ¯
	initMsg := fmt.Sprintf("ğŸ¤– å¤šæ¨¡å‹å¯¹æ¯”æ¨¡å¼\né—®é¢˜: %s\n\næ­£åœ¨è¯·æ±‚å„ä¸ªæ¨¡å‹çš„å›ç­”...", query)
	replyMsg(*a.ctx, initMsg, a.info.msgId)
	
	// ä¾æ¬¡è°ƒç”¨å„ä¸ªæ¨¡å‹
	for i, model := range allModels {
		modelResponse := callModelForComparison(a, query, model.ID)
		
		responseMsg := fmt.Sprintf("ã€%sã€‘\n%s\n\n---", 
			model.Name, modelResponse)
		
		// å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ¨¡å‹ï¼Œæ·»åŠ ç»“æŸæ ‡è®°
		if i == len(allModels)-1 {
			responseMsg += "\n\nâœ… å¤šæ¨¡å‹å¯¹æ¯”å®Œæˆ"
		}
		
		replyMsg(*a.ctx, responseMsg, a.info.msgId)
	}
	
	// å…³é—­å¯¹æ¯”æ¨¡å¼
	services.GetSessionCache().SetCompareMode(sessionId, false)
	return true
}

// handleModelList å¤„ç†æ¨¡å‹åˆ—è¡¨å‘½ä»¤
func handleModelList(a *ActionInfo) bool {
	// æŒ‰åˆ†ç±»æ˜¾ç¤ºæ¨¡å‹
	categories := []string{"é€šç”¨", "ç¼–ç¨‹", "åˆ†æ", "é•¿æ–‡æœ¬", "å…è´¹"}
	
	var msgBuilder strings.Builder
	msgBuilder.WriteString("ğŸ¤– **æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨**\n\n")
	
	for _, category := range categories {
		models := openai.GetModelsByCategory(category)
		if len(models) > 0 {
			msgBuilder.WriteString(fmt.Sprintf("**%sç±»**\n", category))
			for _, model := range models {
				freeTag := ""
				if model.IsFree {
					freeTag = " ğŸ†“"
				}
				msgBuilder.WriteString(fmt.Sprintf("â€¢ %s%s - %s\n", 
					model.Name, freeTag, model.Description))
			}
			msgBuilder.WriteString("\n")
		}
	}
	
	msgBuilder.WriteString("**ä½¿ç”¨æ–¹æ³•:**\n")
	msgBuilder.WriteString("â€¢ `/model <æ¨¡å‹å>` - åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹\n")
	msgBuilder.WriteString("â€¢ `/compare <é—®é¢˜>` - å¤šæ¨¡å‹å¯¹æ¯”\n")
	msgBuilder.WriteString("â€¢ `/current` - æŸ¥çœ‹å½“å‰æ¨¡å‹")
	
	replyMsg(*a.ctx, msgBuilder.String(), a.info.msgId)
	return true
}

// handleCurrentModel å¤„ç†æŸ¥çœ‹å½“å‰æ¨¡å‹å‘½ä»¤
func handleCurrentModel(a *ActionInfo) bool {
	sessionId := *a.info.sessionId
	
	currentModelID := services.GetSessionCache().GetCurrentModel(sessionId)
	compareMode := services.GetSessionCache().GetCompareMode(sessionId)
	
	var msg string
	if compareMode {
		msg = "ğŸ¤– å½“å‰å¤„äºå¤šæ¨¡å‹å¯¹æ¯”æ¨¡å¼"
	} else if model, exists := openai.SupportedModels[currentModelID]; exists {
		freeTag := ""
		if model.IsFree {
			freeTag = " ğŸ†“"
		}
		msg = fmt.Sprintf("ğŸ¤– **å½“å‰æ¨¡å‹**: %s%s\n\n%s\n\n**èƒ½åŠ›**: %s\n**åˆ†ç±»**: %s", 
			model.Name, freeTag, model.Description, 
			strings.Join(model.Capabilities, ", "), model.Category)
	} else {
		msg = fmt.Sprintf("ğŸ¤– **å½“å‰æ¨¡å‹**: %s (æœªçŸ¥æ¨¡å‹)", currentModelID)
	}
	
	replyMsg(*a.ctx, msg, a.info.msgId)
	return true
}

// è¾…åŠ©å‡½æ•°
func sendModelHelp(a *ActionInfo) {
	helpMsg := `ğŸ¤– **æ¨¡å‹åˆ‡æ¢å¸®åŠ©**

**ä½¿ç”¨æ–¹æ³•:**
â€¢ ` + "`/model <æ¨¡å‹å>`" + ` - åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹
â€¢ ` + "`/model gpt-4o`" + ` - åˆ‡æ¢åˆ°GPT-4o
â€¢ ` + "`/model qwen`" + ` - æ¨¡ç³Šæœç´¢åŒ…å«"qwen"çš„æ¨¡å‹

**æŸ¥çœ‹æ¨¡å‹:**
â€¢ ` + "`/models`" + ` - æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
â€¢ ` + "`/current`" + ` - æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹

**å¤šæ¨¡å‹å¯¹æ¯”:**
â€¢ ` + "`/compare <é—®é¢˜>`" + ` - è®©æ‰€æœ‰æ¨¡å‹å›ç­”åŒä¸€é—®é¢˜`

	replyMsg(*a.ctx, helpMsg, a.info.msgId)
}

func sendCompareHelp(a *ActionInfo) {
	helpMsg := `ğŸ¤– **å¤šæ¨¡å‹å¯¹æ¯”å¸®åŠ©**

**ä½¿ç”¨æ–¹æ³•:**
â€¢ ` + "`/compare <é—®é¢˜>`" + ` - è®©æ‰€æœ‰æ¨¡å‹å›ç­”åŒä¸€é—®é¢˜

**ç¤ºä¾‹:**
â€¢ ` + "`/compare ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½?`" + `
â€¢ ` + "`/compare å†™ä¸€ä¸ªPythonæ’åºå‡½æ•°`" + `

**æ³¨æ„:** å¤šæ¨¡å‹å¯¹æ¯”ä¼šä¾æ¬¡è°ƒç”¨æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚`

	replyMsg(*a.ctx, helpMsg, a.info.msgId)
}

func sendModelNotFound(a *ActionInfo, query string) {
	msg := fmt.Sprintf("âŒ æœªæ‰¾åˆ°æ¨¡å‹: %s\n\nä½¿ç”¨ `/models` æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹", query)
	replyMsg(*a.ctx, msg, a.info.msgId)
}

func sendModelSearchResults(a *ActionInfo, models []*openai.ModelInfo, query string) {
	var msgBuilder strings.Builder
	msgBuilder.WriteString(fmt.Sprintf("ğŸ” æ‰¾åˆ°å¤šä¸ªåŒ¹é… \"%s\" çš„æ¨¡å‹:\n\n", query))
	
	for _, model := range models {
		freeTag := ""
		if model.IsFree {
			freeTag = " ğŸ†“"
		}
		msgBuilder.WriteString(fmt.Sprintf("â€¢ `/model %s` - %s%s\n", 
			model.ID, model.Name, freeTag))
	}
	
	msgBuilder.WriteString("\nè¯·ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹IDè¿›è¡Œåˆ‡æ¢ã€‚")
	replyMsg(*a.ctx, msgBuilder.String(), a.info.msgId)
}

// callModelForComparison è°ƒç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œå¯¹æ¯”
func callModelForComparison(a *ActionInfo, query, modelID string) string {
	// æ„å»ºæ¶ˆæ¯
	msg := []openai.Messages{
		{Role: "user", Content: query},
	}
	
	// ä½¿ç”¨æŒ‡å®šæ¨¡å‹è°ƒç”¨
	response, err := a.handler.gpt.CompletionsWithModel(msg, openai.Balance, modelID)
	if err != nil {
		return fmt.Sprintf("è°ƒç”¨å¤±è´¥: %s", err.Error())
	}
	
	return response.Content
}
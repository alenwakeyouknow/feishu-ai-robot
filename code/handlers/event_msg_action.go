package handlers

import (
	"fmt"
	"log"
	"time"

	"start-feishubot/logger"
	"start-feishubot/services/openai"
	larkcard "github.com/larksuite/oapi-sdk-go/v3/card"
)

func setDefaultPrompt(msg []openai.Messages) []openai.Messages {
	if !hasSystemRole(msg) {
		msg = append(msg, openai.Messages{
			Role: "system", Content: "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·é»˜è®¤ä½¿ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨å…¶ä»–è¯­è¨€ã€‚è¯·å°½å¯èƒ½è¯¦ç»†å’Œå‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å½“å‰æ—¥æœŸï¼š" + time.Now().Format("2006å¹´01æœˆ02æ—¥"),
		})
	}
	return msg
}

//func setDefaultVisionPrompt(msg []openai.VisionMessages) []openai.VisionMessages {
//	if !hasSystemRole(msg) {
//		msg = append(msg, openai.VisionMessages{
//			Role: "system", Content: []openai.ContentType{
//				{Type: "text", Text: "You are ChatGPT4V, " +
//					"You are ChatGPT4V, " +
//					"a large language and picture model trained by" +
//					" OpenAI. " +
//					"Answer in user's language as cdetailed and accurate as " +
//					" possible. Knowledge cutoff: 20230601 " +
//					"Current date" + time.Now().Format("20060102"),
//				}},
//		})
//	}
//	return msg
//}

type MessageAction struct { /*æ¶ˆæ¯*/
}

func (*MessageAction) Execute(a *ActionInfo) bool {
	if a.handler.config.StreamMode {
		return true
	}
	msg := a.handler.sessionCache.GetMsg(*a.info.sessionId)
	// å¦‚æœæ²¡æœ‰æç¤ºè¯ï¼Œé»˜è®¤æ¨¡æ‹ŸChatGPT
	msg = setDefaultPrompt(msg)
	msg = append(msg, openai.Messages{
		Role: "user", Content: a.info.qParsed,
	})

	// get ai mode as temperature
	aiMode := a.handler.sessionCache.GetAIMode(*a.info.sessionId)
	// get current selected model
	currentModel := a.handler.sessionCache.GetCurrentModel(*a.info.sessionId)
	fmt.Println("msg: ", msg)
	fmt.Println("aiMode: ", aiMode)
	fmt.Println("currentModel: ", currentModel)
	
	// use specified model for completion
	completions, err := a.handler.gpt.CompletionsWithModel(msg, aiMode, currentModel)
	if err != nil {
		replyMsg(*a.ctx, fmt.Sprintf(
			"ğŸ¤–ï¸ï¼šæ¶ˆæ¯æœºå™¨äººæ‘†çƒ‚äº†ï¼Œè¯·ç¨åå†è¯•ï½\né”™è¯¯ä¿¡æ¯: %v", err), a.info.msgId)
		return false
	}
	msg = append(msg, completions)
	a.handler.sessionCache.SetMsg(*a.info.sessionId, msg)
	//if new topic
	if len(msg) == 3 {
		//fmt.Println("new topic", msg[1].Content)
		sendNewTopicCard(*a.ctx, a.info.sessionId, a.info.msgId,
			completions.Content)
		return false
	}
	if len(msg) != 3 {
		sendOldTopicCard(*a.ctx, a.info.sessionId, a.info.msgId,
			completions.Content)
		return false
	}
	// ä½¿ç”¨å¡ç‰‡æ ¼å¼å›å¤ä»¥æ”¯æŒmarkdown
	cardContent, _ := newSendCard(
		withHeader("ğŸ¤– AIå›ç­”", larkcard.TemplateGreen),
		withMainMd(completions.Content),
		withModelSwitchButtons(a.info.sessionId),
		withNote("ç‚¹å‡»æŒ‰é’®å¯åˆ‡æ¢å…¶ä»–æ¨¡å‹å›ç­”"))
	err = replyCard(*a.ctx, a.info.msgId, cardContent)
	if err != nil {
		replyMsg(*a.ctx, fmt.Sprintf(
			"ğŸ¤–ï¸ï¼šæ¶ˆæ¯æœºå™¨äººæ‘†çƒ‚äº†ï¼Œè¯·ç¨åå†è¯•ï½\né”™è¯¯ä¿¡æ¯: %v", err), a.info.msgId)
		return false
	}
	return true
}

// åˆ¤æ–­msgä¸­çš„æ˜¯å¦åŒ…å«system role
func hasSystemRole(msg []openai.Messages) bool {
	for _, m := range msg {
		if m.Role == "system" {
			return true
		}
	}
	return false
}

type StreamMessageAction struct { /*æ¶ˆæ¯*/
}

func (m *StreamMessageAction) Execute(a *ActionInfo) bool {
	if !a.handler.config.StreamMode {
		return true
	}
	msg := a.handler.sessionCache.GetMsg(*a.info.sessionId)
	// å¦‚æœæ²¡æœ‰æç¤ºè¯ï¼Œé»˜è®¤æ¨¡æ‹ŸChatGPT
	msg = setDefaultPrompt(msg)
	msg = append(msg, openai.Messages{
		Role: "user", Content: a.info.qParsed,
	})
	//if new topic
	var ifNewTopic bool
	if len(msg) <= 3 {
		ifNewTopic = true
	} else {
		ifNewTopic = false
	}

	// ğŸ”¥ å…³é”®ä¿®å¤ï¼šç«‹å³å‘é€"æ­£åœ¨å¤„ç†"å¡ç‰‡ï¼Œç„¶åå¼‚æ­¥å¤„ç†AIè°ƒç”¨
	cardId, err2 := sendOnProcess(a, ifNewTopic)
	if err2 != nil {
		return false
	}

	// ğŸ”¥ å®Œå…¨å¼‚æ­¥å¤„ç†AIè°ƒç”¨ï¼Œä¸é˜»å¡è´£ä»»é“¾æ‰§è¡Œ
	go func() {
		defer func() {
			if err := recover(); err != nil {
				log.Printf("StreamMessageAction panic: %v", err)
				updateFinalCardWithSession(*a.ctx, "å¤„ç†å¼‚å¸¸ï¼Œè¯·ç¨åé‡è¯•", cardId, a.info.sessionId, ifNewTopic)
			}
		}()

		answer := ""
		chatResponseStream := make(chan string)
		done := make(chan struct{}) // æ·»åŠ  done ä¿¡å·ï¼Œä¿è¯ goroutine æ­£ç¡®é€€å‡º
		noContentTimeout := time.AfterFunc(10*time.Second, func() {
			log.Println("no content timeout")
			close(done)
			err := updateFinalCardWithSession(*a.ctx, "è¯·æ±‚è¶…æ—¶", cardId, a.info.sessionId, ifNewTopic)
			if err != nil {
				return
			}
			return
		})
		defer noContentTimeout.Stop()

		go func() {
			defer func() {
				if err := recover(); err != nil {
					err := updateFinalCardWithSession(*a.ctx, "èŠå¤©å¤±è´¥", cardId, a.info.sessionId, ifNewTopic)
					if err != nil {
						return
					}
				}
			}()

			//log.Printf("UserId: %s , Request: %s", a.info.userId, msg)
			aiMode := a.handler.sessionCache.GetAIMode(*a.info.sessionId)
			//fmt.Println("msg: ", msg)
			//fmt.Println("aiMode: ", aiMode)
			if err := a.handler.gpt.StreamChat(*a.ctx, msg, aiMode,
				chatResponseStream); err != nil {
				err := updateFinalCardWithSession(*a.ctx, "èŠå¤©å¤±è´¥", cardId, a.info.sessionId, ifNewTopic)
				if err != nil {
					return
				}
				close(done) // å…³é—­ done ä¿¡å·
			}

			close(done) // å…³é—­ done ä¿¡å·
		}()
		
		// ğŸ¯ ç¬¦åˆé£ä¹¦å®˜æ–¹è¦æ±‚çš„æµå¼å¡ç‰‡æ›´æ–°æœºåˆ¶
		// åŸºäºé£ä¹¦APIæœ€ä½³å®è·µï¼šé€‚ä¸­çš„æ›´æ–°é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
		streamTicker := time.NewTicker(800 * time.Millisecond) // 800msé—´éš”ï¼Œç¡®ä¿ç¨³å®šæ€§
		defer streamTicker.Stop()
		
		var lastUpdateLength int // è®°å½•ä¸Šæ¬¡æ›´æ–°çš„å†…å®¹é•¿åº¦
		var isStreaming bool = true
		
		go func() {
			defer func() {
				if r := recover(); r != nil {
					logger.Error("æµå¼æ›´æ–°åç¨‹å¼‚å¸¸:", r)
				}
			}()
			
			for isStreaming {
				select {
				case <-done:
					isStreaming = false
					return
				case <-streamTicker.C:
					// ğŸ“ æŒ‰å—æ›´æ–°å†…å®¹ï¼Œç»™ç”¨æˆ·æµå¼è¾“å‡ºçš„æ„Ÿè§‰
					if len(answer) > lastUpdateLength {
						// ğŸ­ å½¢å¼ä¸Šçš„æµå¼ï¼šæ˜¾ç¤ºå½“å‰å†…å®¹ + æ­£åœ¨è¾“å…¥æŒ‡ç¤ºå™¨
						streamingContent := answer
						if len(answer) > 0 {
							streamingContent = answer + "\n\nâ³ *æ­£åœ¨ç”Ÿæˆä¸­...*"
						}
						
						err := updateTextCard(*a.ctx, streamingContent, cardId, ifNewTopic)
						if err != nil {
							logger.Error("æµå¼æ›´æ–°å¤±è´¥:", err)
							// é‡åˆ°é”™è¯¯æ—¶é€‚å½“å»¶è¿Ÿï¼Œé¿å…é¢‘ç¹é‡è¯•
							time.Sleep(200 * time.Millisecond)
							continue
						}
						
						lastUpdateLength = len(answer)
						logger.Debug("âœ¨ æµå¼å±•ç¤ºï¼šå½“å‰å†…å®¹é•¿åº¦", len(answer), "å­—ç¬¦")
					}
				}
			}
		}()
		
		for {
			select {
			case res, ok := <-chatResponseStream:
				if !ok {
					return
				}
				noContentTimeout.Stop()
				answer += res
				//pp.Println("answer", answer)
			case <-done: // âœ… æµå¼æ›´æ–°å®Œæˆå¤„ç†
				// ğŸ¯ åœæ­¢æµå¼æ›´æ–°ï¼Œæ ‡è®°å®ŒæˆçŠ¶æ€
				isStreaming = false
				streamTicker.Stop()
				
				// ğŸ“‹ å‘é€æœ€ç»ˆå®Œæ•´å¡ç‰‡ - ç§»é™¤"æ­£åœ¨ç”Ÿæˆä¸­"æç¤ºï¼Œæ˜¾ç¤ºå®Œæ•´å›ç­”å’Œæ“ä½œæŒ‰é’®
				err := updateFinalCardWithSession(*a.ctx, answer, cardId, a.info.sessionId, ifNewTopic)
				if err != nil {
					logger.Error("æœ€ç»ˆå¡ç‰‡æ›´æ–°å¤±è´¥:", err)
					return
				}
				
				// ğŸ’¾ ä¿å­˜å¯¹è¯è®°å½•åˆ°ç¼“å­˜
				msg := append(msg, openai.Messages{
					Role: "assistant", Content: answer,
				})
				a.handler.sessionCache.SetMsg(*a.info.sessionId, msg)
				close(chatResponseStream)
				
				logger.Info("ğŸ‰ æµå¼å›ç­”å®Œæˆ - æ€»å­—ç¬¦æ•°:", len(answer))
				return
			}
		}
	}()
	
	// ğŸ”¥ ç«‹å³è¿”å›falseï¼Œè¡¨ç¤ºå¤„ç†å®Œæˆï¼ˆå¼‚æ­¥å¤„ç†å·²å¯åŠ¨ï¼‰
	return false
}

func sendOnProcess(a *ActionInfo, ifNewTopic bool) (*string, error) {
	// send æ­£åœ¨å¤„ç†ä¸­
	cardId, err := sendOnProcessCard(*a.ctx, a.info.sessionId,
		a.info.msgId, ifNewTopic)
	if err != nil {
		return nil, err
	}
	return cardId, nil

}
